{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMUO6j0/FHcMpvLTfKtrUBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalmysore/AI/blob/main/Examples_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhpON4FzrNKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fa63c4-1af2-43a2-d0de-7b1ca73c3c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "\n",
        "prompt = (\n",
        "    \"In a secret meeting it was decided to serve 20% of the guest with non spicy bland food and other 80% with very spicy food, after the experiment it was discovered that\"\n",
        ")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=100,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "print(\"Generated : \"+gen_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1ZWRi6jAIlU",
        "outputId": "a40fe5bb-e8aa-43b2-adbb-c02c1a34e1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated : In a secret meeting it was decided to serve 20% of the guest with non spicy bland food and other 80% with very spicy food, after the experiment it was discovered that the non spicy food had no effect. But in the second time the results were different, and spicy food with pepper and heat was more effective than bland food with oil and spices.\n",
            "\n",
            "This experiment can also be used to prove the existence of some life form on other planets, which may have developed a taste for spice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "DzuzhWwC0cc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pl = pipeline(task='text-generation',model='EleutherAI/gpt-j-6B')"
      ],
      "metadata": {
        "id": "bqg8Vf12Isne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "#Tokenization: The tokenizer is used to tokenize the input sentence \"Hello, my dog is cute,\"\n",
        "#resulting in a dictionary called inputs that contains the tokenized input tensors.\n",
        "\n",
        "#Model Forward Pass: The model is then called with the tokenized inputs. The **inputs syntax is a way to unpack the contents of the inputs dictionary into keyword arguments.\n",
        "#This is equivalent to calling model(input_ids=..., attention_mask=..., ...) with the individual tensors.\n",
        "\n",
        "#Outputs: The outputs variable now holds the output of the model, which typically includes various information depending on the model architecture.\n",
        "# For language models like GPT, one of the key outputs is usually the last hidden states.\n",
        "\n",
        "# Print the NumPy array\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
        "model = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
        "\n",
        "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "# Convert the tensor to a NumPy array\n",
        "last_hidden_states_np = last_hidden_states.detach().numpy()\n",
        "print(\"Last Hidden States (NumPy Array):\")\n",
        "\n",
        "#print(last_hidden_states_np)\n",
        "decoded_text = tokenizer.decode(inputs[\"input_ids\"][0])\n",
        "\n",
        "# Print the original input text and the decoded text\n",
        "print(\"Original Input Text:\", \"Hello, my dog is cute\")\n",
        "print(\"Decoded Text from Token IDs:\", decoded_text)"
      ],
      "metadata": {
        "id": "AfQk4ywH0Dai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='openai-gpt')\n",
        "set_seed(42)\n",
        "generator(\"the dog jumps over the \", max_length=10, num_return_sequences=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yG2JnsT5K4Z",
        "outputId": "0440a0a0-5522-4251-eea3-295b8ca32602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'the dog jumps over the  top of the porch.'},\n",
              " {'generated_text': 'the dog jumps over the  edge of the cliff and'},\n",
              " {'generated_text': 'the dog jumps over the  hood, \" the one'},\n",
              " {'generated_text': 'the dog jumps over the  fence and sprints over'},\n",
              " {'generated_text': 'the dog jumps over the  fence and lands in front'}]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}